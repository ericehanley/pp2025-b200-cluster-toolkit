# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---

blueprint_name: slurm-custom-img-cluster

vars:
  deployment_name: # supply deployment name
  project_id: # supply project ID
  region: # supply region
  zone: # supply zone
  a4h_cluster_size: # supply cluster size
  # Image settings
  base_image:
    project: ubuntu-os-accelerator-images
    image: ubuntu-accelerator-2204-amd64-with-nvidia-570-v20250722
  image_build_machine_type: n2-standard-16
  build_slurm_from_git_ref: 6.10.0
  # Cluster env settings
  # net0 and filestore ranges must not overlap
  net0_range: 192.168.0.0/19
  filestore_ip_range: 192.168.32.0/24
  net1_range: 192.168.64.0/18
  rdma_net_range: 192.168.128.0/18
  # Cluster Settings
  local_ssd_mountpoint: /mnt/localssd
  instance_image:
    project: $(vars.project_id)
    family: $(vars.deployment_name)-u22
  disk_size_gb: 100
  nccl_plugin_version: v1.0.6
  benchmark_dir: $(ghpc_stage("system_benchmarks"))
  base_network_name: $(vars.deployment_name)

  #Provisioning models (set to true or fill in reservation name, pick only one)
  a4h_reservation_name: "" # supply reservation name
  a4h_dws_flex_enabled: false
  a4h_enable_spot_vm: false

deployment_groups:
- group: image-env
  modules:
  - id: slurm-image-network
    source: modules/network/vpc
    settings:
      network_name: $(vars.base_network_name)-net

  - id: slurm-build-script
    source: modules/scripts/startup-script
    settings:
      install_ansible: true
      docker:
        enabled: true
        world_writable: true
      runners:
      # --- STAGE 1: Install all system software and Slurm FIRST ---
      - type: data
        destination: /etc/apt/preferences.d/block-broken-nvidia-container
        content: |
          Package: nvidia-container-toolkit nvidia-container-toolkit-base libnvidia-container-tools libnvidia-container1
          Pin: version 1.17.7-1
          Pin-Priority: 100
      - type: ansible-local
        destination: hold-nvidia-packages.yml
        content: |
          ---
          - name: Hold nvidia packages
            hosts: all
            become: true
            vars:
              nvidia_packages_to_hold:
              - libnvidia-cfg1-*-server
              - libnvidia-compute-*-server
              - libnvidia-nscq-*
              - nvidia-compute-utils-*-server
              - nvidia-fabricmanager-*
              - nvidia-utils-*-server
            tasks:
            - name: Hold nvidia packages
              ansible.builtin.command:
                argv:
                - apt-mark
                - hold
                - "{{ item }}"
              loop: "{{ nvidia_packages_to_hold }}"
      - type: data
        destination: /var/tmp/slurm_vars.json
        content: |
          { "reboot": false, "install_cuda": false, "install_gcsfuse": true,
            "install_lustre": false, "install_managed_lustre": false,
            "install_nvidia_repo": true, "install_ompi": true,
            "allow_kernel_upgrades": false, "monitoring_agent": "cloud-ops" }
      - type: shell
        destination: install_slurm.sh
        content: |
          #!/bin/bash
          set -e -o pipefail
          ansible-pull \
              -U https://github.com/GoogleCloudPlatform/slurm-gcp -C $(vars.build_slurm_from_git_ref) \
              -i localhost, --limit localhost --connection=local \
              -e @/var/tmp/slurm_vars.json \
              ansible/playbook.yml || true
      - type: data
        destination: /etc/security/limits.d/99-unlimited.conf
        content: |
          * - memlock unlimited
          * - nproc unlimited
          * - stack unlimited
          * - nofile 1048576
          * - cpu unlimited
          * - rtprio unlimited
      - type: ansible-local
        destination: install_cuda_dcgm.yml
        content: |
          ---
          - name: Install CUDA & DCGM
            hosts: all
            become: true
            vars:
              distribution: "{{ ansible_distribution | lower }}{{ ansible_distribution_version | replace('.','') }}"
              cuda_repo_url: https://developer.download.nvidia.com/compute/cuda/repos/{{ distribution }}/x86_64/cuda-keyring_1.1-1_all.deb
            tasks:
            - name: Download NVIDIA repository package
              ansible.builtin.get_url: { url: "{{ cuda_repo_url }}", dest: "/tmp/cuda-keyring.deb" }
            - name: Install NVIDIA repository package
              ansible.builtin.apt: { deb: "/tmp/cuda-keyring.deb", state: present }
            - name: Install CUDA & DCGM
              ansible.builtin.apt: { name: ["cuda-toolkit-12-8", "datacenter-gpu-manager-4-cuda12"], update_cache: true }
      - type: ansible-local
        destination: install_ibverbs_utils.yml
        content: |
          ---
          - name: Install ibverbs-utils
            hosts: all
            become: true
            tasks:
            - name: Install Linux Modules Extra
              ansible.builtin.package: { name: ibverbs-utils, state: present }
      - type: data
        destination: /etc/enroot/enroot.conf
        content: |
          ENROOT_CONFIG_PATH     ${HOME}/.enroot

      # --- STAGE 2: Install Python and packages into an ISOLATED location ---
      - type: ansible-local
        destination: install_python.yml
        content: |
          ---
          - name: Install Python 3.11 and Git
            hosts: all
            become: true
            tasks:
            - name: Update apt cache
              ansible.builtin.apt: { update_cache: true }
            - name: Install prerequisites for adding repositories
              ansible.builtin.apt: { name: software-properties-common, state: present }
            - name: Add deadsnakes PPA for modern Python versions
              ansible.builtin.apt_repository: { repo: ppa:deadsnakes/ppa, state: present }
            - name: Install Python 3.11, pip, and Git
              ansible.builtin.apt:
                name: ['python3.11', 'python3.11-venv', 'python3-pip', 'git']
                state: present
      - type: data
        destination: /tmp/requirements.txt
        content: |
          numpy>=2.0.2
          gymnasium==0.29.1 # -- loosen
          lightning>=2.5.0.post0
          torchvision>=0.21.0
          deepspeed>=0.14.5
          peft>=0.15.2
          transformers[torch]>=4.49.0
          tensorflow>=2.18.0 # Confirm TF version
          tensorflow-text>=2.18.1 # Adjusted to match TF version
          keras-nlp>=0.19.3
          tf-keras>=2.18.0 # Adjusted to match TF version
          ray>=2.43.0
          cut-cross-entropy
          torch>=2.7.0 # wheel file? torch-2.7.0+cu126-cp311-cp311-manylinux_2_28_x86_64.whl
          tensordict==0.8.3 # wheel file? tensordict-0.7.2-cp311-cp311-manylinux1_x86_64.whl
          torchrl==0.8.1 
          torchtune==0.6.0
          torch-tb-profiler>=0.4.3
          torchao>=0.9.0
          compressed-tensors>=0.9.3
          hydra-core>=1.3.2
          pretty-midi>=0.2.10
          tensorboard>=2.18.0 # Adjusted to match TF version
          ipykernel>=6.29.5
          hatchling>=1.27.0
          editables>=0.5
          psutil>=5.9.0
          tenacity>=9.1.2
          pydantic>=2.11.3
          pydantic-core>=2.33.1
          huggingface-hub>=0.30.0
          lm-format-enforcer>=0.10.11
          mistral-common>=1.5.4
          numba>=0.61.2
          setuptools>=78.1.0
          cmake>=3.25.0,<=3.26.3
          coverage>=7.6.12
          httptools>=0.6.4
          xgrammar>=0.1.18
          singleton-decorator>=1.0.0
          regex>=2024.11.6
          git+https://github.com/apple/ml-cross-entropy.git
      - type: shell
        destination: install_python_packages_isolated.sh
        content: |
          #!/bin/bash
          set -e -o pipefail
          echo "--- Creating isolated Python environment in /opt/apps/python/3.11 ---"
          mkdir -p /opt/apps/python
          python3.11 -m venv /opt/apps/python/3.11
          echo "--- Installing packages into isolated environment ---"
          /opt/apps/python/3.11/bin/pip install --upgrade pip
          /opt/apps/python/3.11/bin/pip install --no-cache-dir -r /tmp/requirements.txt
          echo "--- Package installation complete ---"

      # --- STAGE 3: Make the isolated Python the system default for users ---
      - type: data
        destination: /etc/profile.d/00-python-app-env.sh
        content: |
          #!/bin/bash
          export PATH="/opt/apps/python/3.11/bin:$PATH"

- group: image
  modules:
  - id: slurm-a4high-image
    source: modules/packer/custom-image
    kind: packer
    settings:
      disk_size: $(vars.disk_size_gb)
      machine_type: $(vars.image_build_machine_type)
      source_image: $(vars.base_image.image)
      source_image_project_id: [$(vars.base_image.project)]
      image_family: $(vars.instance_image.family)
      omit_external_ip: false
      # Unattended upgrades are disabled in this blueprint so that software does not
      # get updated daily and lead to potential instability in the cluster environment.
      #
      # Unattended Upgrades installs available security updates from the Ubuntu
      # security pocket for installed packages daily by default. Administrators who
      # disable this feature assume all responsibility for manually reviewing and
      # patching their systems against vulnerabilities.
      #
      # To enable unattended upgrades, please remove this section.
      metadata:
        user-data: |
          #cloud-config
          write_files:
          - path: /etc/apt/apt.conf.d/20auto-upgrades
            permissions: '0644'
            owner: root
            content: |
              APT::Periodic::Update-Package-Lists "0";
              APT::Periodic::Unattended-Upgrade "0";
    use:
    - slurm-image-network
    - slurm-build-script

- group: cluster-env
  modules:
  - id: a4high-slurm-net-0
    source: modules/network/vpc
    settings:
      network_name: $(vars.base_network_name)-net-0
      mtu: 8896
      enable_internal_traffic: false # Setting firewall below instead
      subnetworks:
      - subnet_name: $(vars.base_network_name)-sub-0
        subnet_region: $(vars.region)
        subnet_ip: $(vars.net0_range)
      firewall_rules:
      - name: $(vars.base_network_name)-internal-0
        ranges: [$(vars.net0_range)]
        allow:
        - protocol: tcp
        - protocol: udp
        - protocol: icmp

  - id: a4high-slurm-net-1
    source: modules/network/vpc
    settings:
      network_name: $(vars.base_network_name)-net-1
      mtu: 8896
      enable_internal_traffic: false # Setting firewall below instead
      subnetworks:
      - subnet_name: $(vars.base_network_name)-sub-1
        subnet_region: $(vars.region)
        subnet_ip: $(vars.net1_range)
      firewall_rules:
      - name: $(vars.base_network_name)-internal-1
        ranges: [$(vars.net1_range)]
        allow:
        - protocol: tcp
        - protocol: udp
        - protocol: icmp

  - id: a4high-slurm-rdma-net
    source: modules/network/gpu-rdma-vpc
    settings:
      network_name: $(vars.base_network_name)-rdma-net
      network_profile: https://www.googleapis.com/compute/beta/projects/$(vars.project_id)/global/networkProfiles/$(vars.zone)-vpc-roce
      network_routing_mode: REGIONAL
      subnetworks_template:
        name_prefix: $(vars.base_network_name)-mrdma-sub
        count: 8
        ip_range: $(vars.rdma_net_range)
        region: $(vars.region)

  - id: homefs
    source: modules/file-system/filestore
    use:
    - a4high-slurm-net-0
    settings:
      filestore_tier: HIGH_SCALE_SSD
      size_gb: 10240
      local_mount: /home
      reserved_ip_range: $(vars.filestore_ip_range)
      deletion_protection:
        enabled: true
        reason: Avoid data loss
    outputs:
    - network_storage

  # - id: private_service_access
  #   source: community/modules/network/private-service-access
  #   use: [a4high-slurm-net-0]

  # To use Managed Lustre as for the shared /home directory:
  # 1. Comment out the filestore block above and the`filestore_ip_range` line in the vars block.
  # 2. Uncomment the managed-lustre and private-service-access blocks
  # 3. Change the value for "install_managed_lustre" in /var/tmp/slurm_vars.json above to true
  # - id: homefs
  #   source: modules/file-system/managed-lustre
  #   use:
  #   - a4high-slurm-net-0
  #   - private_service_access
  #   settings:
  #     size_gib: 18000
  #     name: lustre-instance1
  #     local_mount: /home
  #     remote_mount: lustrefs
  #   outputs:
  #   - network_storage

  # The following four modules create and mount a Cloud Storage Bucket with
  # gcsfuse.  They are optional but recommended for many use cases.
  # (Optional) The following creates a GCS bucket that will be mounted
  # using gcsfuse. If you prefer to use a pre-existing bucket, use the
  # modules/file-system/pre-existing-network-storage module.
  - id: gcs_bucket
    source: community/modules/file-system/cloud-storage-bucket
    settings:
      enable_hierarchical_namespace: true
      local_mount: /gcs
      random_suffix: true
      mount_options: "implicit-dirs,\
                      metadata-cache-negative-ttl-secs=0,\
                      metadata-cache-ttl-secs=-1,\
                      stat-cache-max-size-mb=-1,\
                      type-cache-max-size-mb=-1,\
                      enable-streaming-writes=true,\
                      dir-mode=777,\
                      file-mode=777,\
                      allow_other"

  # (Optional) Create a mount-point optimized for checkpoint writing/reading Uses
  # local-ssd for and enables parallel downloads.  For more information on
  # these flags, see
  # https://github.com/GoogleCloudPlatform/gcsfuse/tree/master/samples/gcsfuse_config
  - id: gcs_checkpoints
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(gcs_bucket.gcs_bucket_name)
      local_mount: /gcs-checkpoints
      fs_type: gcsfuse
      mount_options: "implicit-dirs,\
                      metadata-cache-negative-ttl-secs=0,\
                      metadata-cache-ttl-secs=-1,\
                      stat-cache-max-size-mb=-1,\
                      type-cache-max-size-mb=-1,\
                      file-cache-max-size-mb=-1,\
                      file-cache-cache-file-for-range-read=true,\
                      file-cache-enable-parallel-downloads=true,\
                      cache-dir=/mnt/localssd,\
                      enable-streaming-writes=true,\
                      dir-mode=777,\
                      file-mode=777,\
                      allow_other"

  # (Optional) Create a mount-point optimized for reading training data.
  # For more information on these flags, see
  # https://github.com/GoogleCloudPlatform/gcsfuse/tree/master/samples/gcsfuse_config
  # If your training dataset fits on localssd, then you may want to enable file
  # cache as well, which is done by adding
  # cache-dir=/mnt/localssd,\
  # file-cache-max-size-mb=<DATASET_SIZE>,\
  # file-cache-cache-file-for-range-read=true,\
  # to the mount_options.
  - id: gcs_training_data
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(gcs_bucket.gcs_bucket_name)
      local_mount: /gcs-training-data
      fs_type: gcsfuse
      mount_options: "implicit-dirs,\
                      metadata-cache-negative-ttl-secs=0,\
                      metadata-cache-ttl-secs=-1,\
                      stat-cache-max-size-mb=-1,\
                      type-cache-max-size-mb=-1,\
                      enable-streaming-writes=true,\
                      dir-mode=777,\
                      file-mode=777,\
                      allow_other"

  # (Optional) Create a mount-point optimized for model serving.
  # For more information on these flags, see
  # https://github.com/GoogleCloudPlatform/gcsfuse/tree/master/samples/gcsfuse_config
  - id: gcs_model_serving
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(gcs_bucket.gcs_bucket_name)
      local_mount: /gcs-model-serving
      fs_type: gcsfuse
      mount_options: "implicit-dirs,\
                      cache-dir=/mnt/localssd,\
                      metadata-cache-negative-ttl-secs=0,\
                      metadata-cache-ttl-secs=-1,\
                      stat-cache-max-size-mb=-1,\
                      type-cache-max-size-mb=-1,\
                      file-cache-max-size-mb=-1,\
                      file-cache-cache-file-for-range-read=true,\
                      file-cache-enable-parallel-downloads=true,\
                      dir-mode=777,\
                      file-mode=777,\
                      allow_other"

- group: cluster
  modules:
  - id: a4high_startup
    source: modules/scripts/startup-script
    settings:
      local_ssd_filesystem:
        mountpoint: $(vars.local_ssd_mountpoint)
        permissions: "1777" # must quote numeric filesystem permissions!
      docker:
        enabled: true
        world_writable: true
        daemon_config: |
          {
            "data-root": "$(vars.local_ssd_mountpoint)/docker"
          }
      runners:
      - $(gcs_checkpoints.client_install_runner)
      - $(gcs_checkpoints.mount_runner)
      - $(gcs_training_data.client_install_runner)
      - $(gcs_training_data.mount_runner)
      - $(gcs_model_serving.client_install_runner)
      - $(gcs_model_serving.mount_runner)
      - type: data
        destination: /etc/enroot/enroot.conf
        content: |
          ENROOT_CONFIG_PATH     ${HOME}/.enroot
          ENROOT_RUNTIME_PATH    $(vars.local_ssd_mountpoint)/${UID}/enroot/runtime
          ENROOT_CACHE_PATH      $(vars.local_ssd_mountpoint)/${UID}/enroot/cache
          ENROOT_DATA_PATH       $(vars.local_ssd_mountpoint)/${UID}/enroot/data
          ENROOT_TEMP_PATH       $(vars.local_ssd_mountpoint)/${UID}/enroot
      - type: ansible-local
        destination: nccl_plugin.yml
        content: |
          ---
          - name: Install NCCL plugin for A4 High series
            hosts: all
            become: true
            tasks:
            - name: Add SystemD unit for NCCL plugin installation
              ansible.builtin.copy:
                dest: /etc/systemd/system/nccl-plugin@.service
                mode: 0o0644
                content: |
                  [Unit]
                  After=network-online.target docker.service
                  Before=slurmd.service
                  Requires=docker.service

                  [Service]
                  Type=oneshot
                  ExecStartPre=/usr/bin/rm -rf /usr/local/gib
                  ExecStartPre=/usr/bin/mkdir -p /usr/local/gib
                  ExecStartPre=/snap/bin/gcloud auth configure-docker --quiet us-docker.pkg.dev
                  ExecStart=/usr/bin/docker run --rm --name nccl-gib-installer --volume /usr/local/gib:/var/lib/gib \
                      us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib:%i install --install-nccl
                  ExecStartPost=/usr/bin/chmod -R a+r /usr/local/gib

                  [Install]
                  WantedBy=slurmd.service
              notify:
              - Reload SystemD
            handlers:
            - name: Reload SystemD
              ansible.builtin.systemd:
                daemon_reload: true
            post_tasks:
            - name: Enable NCCL plugin SystemD unit
              ansible.builtin.service:
                name: nccl-plugin@$(vars.nccl_plugin_version).service
                state: started
                enabled: true
      - type: ansible-local
        destination: enable_dcgm.yml
        content: |
          ---
          - name: Enable NVIDIA DCGM on GPU nodes
            hosts: all
            become: true
            vars:
              enable_ops_agent: true
              enable_nvidia_dcgm: true
              enable_nvidia_persistenced: true
            tasks:
            - name: Update Ops Agent configuration
              ansible.builtin.blockinfile:
                path: /etc/google-cloud-ops-agent/config.yaml
                insertafter: EOF
                block: |
                  metrics:
                    receivers:
                      dcgm:
                        type: dcgm
                    service:
                      pipelines:
                        dcgm:
                          receivers:
                            - dcgm
              notify:
              - Restart Google Cloud Ops Agent
            handlers:
            - name: Restart Google Cloud Ops Agent
              ansible.builtin.service:
                name: google-cloud-ops-agent.service
                state: "{{ 'restarted' if enable_ops_agent else 'stopped' }}"
                enabled: "{{ enable_ops_agent }}"
            post_tasks:
            - name: Enable Google Cloud Ops Agent
              ansible.builtin.service:
                name: google-cloud-ops-agent.service
                state: "{{ 'started' if enable_ops_agent else 'stopped' }}"
                enabled: "{{ enable_ops_agent }}"
            - name: Enable NVIDIA DCGM
              ansible.builtin.service:
                name: nvidia-dcgm.service
                state: "{{ 'started' if enable_nvidia_dcgm else 'stopped' }}"
                enabled: "{{ enable_nvidia_dcgm }}"
            - name: Enable NVIDIA Persistence Daemon
              ansible.builtin.service:
                name: nvidia-persistenced.service
                state: "{{ 'started' if enable_nvidia_persistenced else 'stopped' }}"
                enabled: "{{ enable_nvidia_persistenced }}"

  - id: a4high_nodeset
    source: community/modules/compute/schedmd-slurm-gcp-v6-nodeset
    use: [a4high-slurm-net-0, a4high_startup]
    settings:
      bandwidth_tier: gvnic_enabled
      machine_type: a4-highgpu-8g
      enable_public_ips: true
      node_count_static: $(vars.a4h_cluster_size)
      node_count_dynamic_max: 0
      enable_placement: false
      disk_type: hyperdisk-balanced
      on_host_maintenance: TERMINATE

      #Provisioning models
      reservation_name: $(vars.a4h_reservation_name)
      enable_spot_vm: $(vars.a4h_enable_spot_vm)
      dws_flex:
        enabled: $(vars.a4h_dws_flex_enabled)

      advanced_machine_features:
        threads_per_core: null # Use platform default value
      node_conf:
        CoresPerSocket: 56
        SocketsPerBoard: 2
        ThreadsPerCore: 2
      additional_networks:
        $(concat(
          [{
            network=null,
            subnetwork=a4high-slurm-net-1.subnetwork_self_link,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip="",
            stack_type=null,
            access_config=[],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
          a4high-slurm-rdma-net.subnetwork_interfaces
        ))

  - id: a4high_partition
    source: community/modules/compute/schedmd-slurm-gcp-v6-partition
    use:
    - a4high_nodeset
    settings:
      exclusive: false
      partition_name: a4high
      is_default: true
      partition_conf:
        OverSubscribe: EXCLUSIVE
        ResumeTimeout: 1200
        SuspendTimeout: 1200

  - id: slurm_login
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-login
    use: [a4high-slurm-net-0]
    settings:
      disk_size_gb: 300
      enable_login_public_ips: true
      machine_type: n2-standard-8

  - id: controller_startup
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        destination: stage_scripts.sh
        content: |
          #!/bin/bash
          SLURM_ROOT=/opt/apps/adm/slurm
          PARTITION_NAME=$(a4high_partition.partitions[0].partition_name)
          mkdir -m 0755 -p "${SLURM_ROOT}/scripts"
          # enable a GPU health check that runs at the completion of all jobs on A4 nodes
          mkdir -p "${SLURM_ROOT}/partition-${PARTITION_NAME}-epilog_slurmd.d"
          ln -s "/slurm/scripts/tools/gpu-test" "${SLURM_ROOT}/partition-${PARTITION_NAME}-epilog_slurmd.d/gpu-test.epilog_slurmd"
          # enable the use of password-free sudo within Slurm jobs on all compute nodes
          # feature is restricted to users with OS Admin Login IAM role
          # https://cloud.google.com/iam/docs/understanding-roles#compute.osAdminLogin
          mkdir -p "${SLURM_ROOT}/prolog_slurmd.d"
          mkdir -p "${SLURM_ROOT}/epilog_slurmd.d"
          curl -s -o "${SLURM_ROOT}/scripts/sudo-oslogin" \
              https://raw.githubusercontent.com/GoogleCloudPlatform/slurm-gcp/master/tools/prologs-epilogs/sudo-oslogin
          chmod 0755 "${SLURM_ROOT}/scripts/sudo-oslogin"
          ln -s "${SLURM_ROOT}/scripts/sudo-oslogin" "${SLURM_ROOT}/prolog_slurmd.d/sudo-oslogin.prolog_slurmd"
          ln -s "${SLURM_ROOT}/scripts/sudo-oslogin" "${SLURM_ROOT}/epilog_slurmd.d/sudo-oslogin.epilog_slurmd"
      - type: data
        destination: /opt/apps/system_benchmarks/run-nccl-tests-via-ramble.sh
        source: $(vars.benchmark_dir)/run-nccl-tests-via-ramble.sh
      - type: data
        destination: /opt/apps/system_benchmarks/README.md
        source: $(vars.benchmark_dir)/README.md

  - id: slurm_controller
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-controller
    use:
    - a4high-slurm-net-0
    - a4high_partition
    - slurm_login
    - homefs
    - gcs_bucket
    settings:
      enable_controller_public_ips: true
      disk_type: pd-extreme
      disk_size_gb: 300
      machine_type: n2-standard-80
      controller_startup_script: $(controller_startup.startup_script)
      enable_external_prolog_epilog: true
